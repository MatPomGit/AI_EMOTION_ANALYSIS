# ==================================================================================
# ANALIZATOR EMOCJI I ZACHOWANIA - G≈Ç√≥wny plik aplikacji
# ==================================================================================
# Ten program analizuje emocje i zachowania cz≈Çowieka na podstawie wideo u≈ºywajƒÖc
# zaawansowanych bibliotek sztucznej inteligencji.
#
# Autor: MatPomGit
# Przeznaczenie: Projekt edukacyjny dla student√≥w uczƒÖcych siƒô AI i Python
# ==================================================================================

# SEKCJA 1: IMPORTOWANIE BIBLIOTEK
# ==================================================================================
# W tej sekcji importujemy wszystkie potrzebne biblioteki (modu≈Çy) do dzia≈Çania programu.
# Ka≈ºda biblioteka dostarcza konkretne funkcjonalno≈õci.

import cv2  # OpenCV - biblioteka do przetwarzania obraz√≥w i wideo
            # cv2 pozwala na: odczyt wideo, manipulacjƒô obrazami, rysowanie na obrazach

import numpy as np  # NumPy - biblioteka do operacji na tablicach numerycznych
                    # np u≈ºywamy do oblicze≈Ñ matematycznych na du≈ºych zbiorach danych

import streamlit as st  # Streamlit - framework do tworzenia aplikacji webowych
                        # st pozwala na szybkie stworzenie interfejsu u≈ºytkownika

from deepface import DeepFace  # DeepFace - biblioteka do rozpoznawania emocji na twarzy
                                # DeepFace wykorzystuje g≈Çƒôbokie sieci neuronowe

import mediapipe as mp  # MediaPipe - biblioteka Google do analizy multimedialnej
                        # mp dostarcza gotowe rozwiƒÖzania do wykrywania twarzy i d≈Çoni

import datetime  # datetime - wbudowana biblioteka do pracy z datƒÖ i czasem
                # U≈ºywamy jej do oznaczania czasu w raportach

import json  # json - wbudowana biblioteka do pracy z formatem JSON
            # JSON to format przechowywania danych, u≈ºywamy go do komunikacji z API

import tempfile  # tempfile - wbudowana biblioteka do tworzenia plik√≥w tymczasowych
                # U≈ºywamy jej do zapisywania przes≈Çanych plik√≥w wideo

import os  # os - wbudowana biblioteka do operacji systemowych
          # U≈ºywamy jej do pracy ze ≈õcie≈ºkami plik√≥w i usuwania plik√≥w

from google import genai  # Google Generative AI - API do generowania analiz przez AI
                          # genai pozwala nam u≈ºywaƒá modeli AI Google (np. Gemini)

# SEKCJA 2: INICJALIZACJA NARZƒòDZI MEDIAPIPE
# ==================================================================================
# MediaPipe oferuje gotowe rozwiƒÖzania (solutions) do r√≥≈ºnych zada≈Ñ.
# Musimy je zainicjalizowaƒá przed u≈ºyciem.

mp_hands = mp.solutions.hands  # RozwiƒÖzanie do wykrywania d≈Çoni
                               # Wykrywa 21 punkt√≥w charakterystycznych na d≈Çoni

mp_face_mesh = mp.solutions.face_mesh  # RozwiƒÖzanie do wykrywania siatki twarzy
                                       # Wykrywa 468 punkt√≥w charakterystycznych na twarzy

mp_drawing = mp.solutions.drawing_utils  # Narzƒôdzia do rysowania punkt√≥w na obrazie
                                         # U≈ºywamy tego do wizualizacji wykrytych punkt√≥w

# SEKCJA 3: ZMIENNE GLOBALNE - PRZECHOWYWANIE DANYCH ANALIZY
# ==================================================================================
# Streamlit u≈ºywa "session_state" do przechowywania danych miƒôdzy kolejnymi
# od≈õwie≈ºeniami strony. To jak "pamiƒôƒá" aplikacji.
# Wszystkie dane zbierane podczas analizy sƒÖ tutaj zapisywane.

# Raport z analizy behawioralnej - lista wszystkich zdarze≈Ñ podczas analizy
if "behavior_report" not in st.session_state:
    st.session_state.behavior_report = []  # Pusta lista na rozpoczƒôcie

# Sumy procentowe wszystkich emocji wykrytych w ka≈ºdej klatce
# Kluczami sƒÖ nazwy emocji, warto≈õciami - suma procent√≥w ze wszystkich klatek
if "emotion_totals" not in st.session_state:
    st.session_state.emotion_totals = {
        "happy": 0,      # Rado≈õƒá
        "sad": 0,        # Smutek
        "angry": 0,      # Z≈Ço≈õƒá
        "surprise": 0,   # Zaskoczenie
        "fear": 0,       # Strach
        "disgust": 0,    # Wstrƒôt
        "neutral": 0     # Neutralno≈õƒá
    }

# Licznik przeanalizowanych klatek wideo
# Potrzebny do obliczenia ≈õredniej warto≈õci emocji
if "frame_count" not in st.session_state:
    st.session_state.frame_count = 0

# Liczniki gest√≥w d≈Çoni wykrytych podczas analizy
if "hand_gesture_count" not in st.session_state:
    st.session_state.hand_gesture_count = {
        "tense": 0,      # Napiƒôte gesty (palce zaci≈õniƒôte)
        "relaxed": 0     # Rozlu≈∫nione gesty (palce rozlu≈∫nione)
    }

# Liczniki kierunku spojrzenia wykrytego podczas analizy
if "eye_direction_count" not in st.session_state:
    st.session_state.eye_direction_count = {
        "left": 0,       # Spojrzenie w lewo
        "right": 0,      # Spojrzenie w prawo
        "center": 0      # Spojrzenie prosto
    }

# Liczniki ruch√≥w g≈Çowy wykrytych podczas analizy
if "head_movement_count" not in st.session_state:
    st.session_state.head_movement_count = {
        "up": 0,         # G≈Çowa w g√≥rƒô
        "down": 0,       # G≈Çowa w d√≥≈Ç
        "still": 0       # G≈Çowa nieruchomo
    }

# Flaga kontrolujƒÖca czy kamera/analiza jest aktywna
# True = analiza trwa, False = analiza zatrzymana
if "camera_running" not in st.session_state:
    st.session_state.camera_running = False


# SEKCJA 4: FUNKCJE POMOCNICZE
# ==================================================================================
# W tej sekcji definiujemy funkcje, kt√≥re wykonujƒÖ konkretne zadania.
# Podzia≈Ç na funkcje sprawia, ≈ºe kod jest bardziej czytelny i ≈Çatwiejszy w utrzymaniu.

# FUNKCJA 1: Logowanie danych do raportu
# ==================================================================================
def log_to_report(mode, analysis):
    """
    Zapisuje dane do raportu behawioralnego z aktualnym znacznikiem czasu.
    
    Parametry:
    ----------
    mode : str
        Tryb analizy (np. "Detective", "Student Behavior", "Interview")
    analysis : str
        Tekst z wynikiem analizy do zapisania w raporcie
        
    Dzia≈Çanie:
    ----------
    1. Pobiera aktualny czas
    2. Formatuje go do czytelnej postaci (RRRR-MM-DD GG:MM:SS)
    3. Dodaje wpis do listy raport√≥w w pamiƒôci aplikacji
    
    Przyk≈Çad u≈ºycia:
    ----------------
    log_to_report("Detective", "Wykryto emocjƒô: rado≈õƒá (85%)")
    """
    # Pobierz aktualnƒÖ datƒô i czas
    timestamp = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    
    # Dodaj sformatowany wpis do listy raport√≥w
    # Format: "2024-01-15 14:30:45 - Detective: Wykryto emocjƒô: rado≈õƒá (85%)"
    st.session_state.behavior_report.append(f"{timestamp} - {mode}: {analysis}")


# FUNKCJA 2: Analiza emocji za pomocƒÖ DeepFace
# ==================================================================================
def analyze_emotion(frame):
    """
    Analizuje emocje widoczne na twarzy w pojedynczej klatce wideo.
    
    Parametry:
    ----------
    frame : numpy.ndarray
        Pojedyncza klatka wideo (obraz) w formacie OpenCV (BGR)
        
    Zwraca:
    -------
    dict
        S≈Çownik z wynikami analizy, gdzie:
        - klucze to nazwy emocji (np. "happy", "sad")
        - warto≈õci to procentowe prawdopodobie≈Ñstwo dla ka≈ºdej emocji (0-100)
        
    Dzia≈Çanie:
    ----------
    1. DeepFace.analyze() przetwarza obraz
    2. Wykrywa twarz na obrazie (je≈õli jest)
    3. Analizuje cechy twarzy (kszta≈Çt ust, oczu, brwi)
    4. Por√≥wnuje z wytrenowanym modelem sieci neuronowej
    5. Zwraca prawdopodobie≈Ñstwa dla ka≈ºdej z 7 emocji
    
    Uwaga:
    ------
    enforce_detection=False sprawia, ≈ºe funkcja nie zg≈Çasza b≈Çƒôdu,
    je≈õli nie wykryje twarzy - zamiast tego zwraca neutralne wyniki
    """
    try:
        # Wywo≈Çaj analizƒô DeepFace na bie≈ºƒÖcej klatce
        # actions=['emotion'] - analizujemy tylko emocje (nie wiek, p≈Çeƒá, rasƒô)
        result = DeepFace.analyze(frame, actions=['emotion'], enforce_detection=False)
        
        # Sprawd≈∫ czy wynik jest prawid≈Çowy i niepusty
        if result and len(result) > 0 and 'emotion' in result[0]:
            # WyciƒÖgnij s≈Çownik z wynikami emocji z wyniku analizy
            # result[0] bo DeepFace mo≈ºe zwracaƒá listƒô wynik√≥w (dla wielu twarzy)
            emotion_scores = result[0]['emotion']
            return emotion_scores
        else:
            # Je≈õli wynik jest pusty, zwr√≥ƒá neutralne warto≈õci
            return {
                "happy": 0.0,
                "sad": 0.0,
                "angry": 0.0,
                "surprise": 0.0,
                "fear": 0.0,
                "disgust": 0.0,
                "neutral": 100.0
            }
    except Exception as e:
        # W przypadku b≈Çƒôdu (np. problem z modelem, uszkodzony obraz)
        # Zwr√≥ƒá neutralne warto≈õci i zaloguj ostrze≈ºenie
        print(f"Ostrze≈ºenie: B≈ÇƒÖd analizy emocji: {e}")
        return {
            "happy": 0.0,
            "sad": 0.0,
            "angry": 0.0,
            "surprise": 0.0,
            "fear": 0.0,
            "disgust": 0.0,
            "neutral": 100.0
        }


# FUNKCJA 3: Analiza gest√≥w d≈Çoni za pomocƒÖ MediaPipe
# ==================================================================================
def analyze_hands(frame, hands):
    """
    Analizuje gesty d≈Çoni widoczne w klatce wideo.
    
    Parametry:
    ----------
    frame : numpy.ndarray
        Pojedyncza klatka wideo (obraz) w formacie OpenCV (BGR)
    hands : mediapipe.solutions.hands.Hands
        Zainicjalizowany obiekt MediaPipe Hands do wykrywania d≈Çoni
        
    Dzia≈Çanie:
    ----------
    1. Konwertuje obraz z BGR (OpenCV) do RGB (MediaPipe)
    2. Wykrywa d≈Çonie i ich punkty charakterystyczne (21 punkt√≥w na d≈Ço≈Ñ)
    3. Dla ka≈ºdej wykrytej d≈Çoni:
       - Mierzy odleg≈Ço≈õƒá miƒôdzy kciukiem a palcem wskazujƒÖcym
       - Je≈õli odleg≈Ço≈õƒá < 0.1: gest napiƒôty (palce zaci≈õniƒôte)
       - Je≈õli odleg≈Ço≈õƒá >= 0.1: gest rozlu≈∫niony (palce rozlu≈∫nione)
    4. Rysuje punkty i po≈ÇƒÖczenia na obrazie (wizualizacja)
    5. Aktualizuje liczniki gest√≥w w pamiƒôci aplikacji
    
    Uwaga:
    ------
    MediaPipe wymaga obrazu w formacie RGB, a OpenCV u≈ºywa BGR,
    dlatego konwertujemy kolory przed przetwarzaniem
    """
    # Konwersja z BGR (format OpenCV) na RGB (format MediaPipe)
    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
    
    # Przetw√≥rz klatkƒô przez MediaPipe Hands
    results = hands.process(frame_rgb)

    # Sprawd≈∫ czy wykryto jakiekolwiek d≈Çonie
    if results.multi_hand_landmarks:
        # Iteruj przez wszystkie wykryte d≈Çonie
        for hand_landmarks in results.multi_hand_landmarks:
            # Pobierz wsp√≥≈Çrzƒôdne kciuka (punkt 4 z 21 punkt√≥w d≈Çoni)
            # HandLandmark.THUMB_TIP to sta≈Ça okre≈õlajƒÖca czubek kciuka
            thumb_tip = hand_landmarks.landmark[mp_hands.HandLandmark.THUMB_TIP]
            
            # Pobierz wsp√≥≈Çrzƒôdne czubka palca wskazujƒÖcego (punkt 8)
            index_tip = hand_landmarks.landmark[mp_hands.HandLandmark.INDEX_FINGER_TIP]
            
            # Oblicz odleg≈Ço≈õƒá miƒôdzy kciukiem a palcem wskazujƒÖcym
            # U≈ºywamy zar√≥wno odleg≈Ço≈õci w poziomie (x) jak i pionie (y)
            distance = abs(thumb_tip.x - index_tip.x) + abs(thumb_tip.y - index_tip.y)

            # Klasyfikacja gestu na podstawie odleg≈Ço≈õci
            if distance < 0.1:
                # Ma≈Ça odleg≈Ço≈õƒá = palce blisko siebie = gest napiƒôty
                st.session_state.hand_gesture_count["tense"] += 1
            else:
                # Du≈ºa odleg≈Ço≈õƒá = palce daleko od siebie = gest rozlu≈∫niony
                st.session_state.hand_gesture_count["relaxed"] += 1

            # Narysuj punkty charakterystyczne d≈Çoni i po≈ÇƒÖczenia miƒôdzy nimi
            # HAND_CONNECTIONS to predefiniowana lista po≈ÇƒÖcze≈Ñ miƒôdzy punktami
            mp_drawing.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)


# FUNKCJA 4: Analiza kierunku spojrzenia i ruch√≥w g≈Çowy
# ==================================================================================
def analyze_face(frame, face_mesh):
    """
    Analizuje kierunek spojrzenia oczu i ruchy g≈Çowy w klatce wideo.
    
    Parametry:
    ----------
    frame : numpy.ndarray
        Pojedyncza klatka wideo (obraz) w formacie OpenCV (BGR)
    face_mesh : mediapipe.solutions.face_mesh.FaceMesh
        Zainicjalizowany obiekt MediaPipe Face Mesh do wykrywania twarzy
        
    Dzia≈Çanie:
    ----------
    1. Konwertuje obraz z BGR na RGB
    2. Wykrywa twarz i jej punkty charakterystyczne (468 punkt√≥w)
    3. Analizuje kierunek oczu:
       - Punkt 33: lewe oko
       - Punkt 263: prawe oko
       - Na podstawie pozycji x okre≈õla kierunek spojrzenia
    4. Analizuje pozycjƒô g≈Çowy:
       - Punkt 4: czubek nosa
       - Na podstawie pozycji y okre≈õla ruch g≈Çowy (g√≥ra/d√≥≈Ç/nieruchomo)
    5. Rysuje siatkƒô twarzy na obrazie (wizualizacja)
    6. Aktualizuje liczniki w pamiƒôci aplikacji
    
    Uwaga:
    ------
    Wsp√≥≈Çrzƒôdne sƒÖ znormalizowane (0.0 - 1.0):
    - x=0.0 to lewa krawƒôd≈∫ obrazu, x=1.0 to prawa krawƒôd≈∫
    - y=0.0 to g√≥rna krawƒôd≈∫ obrazu, y=1.0 to dolna krawƒôd≈∫
    """
    # Konwersja z BGR (format OpenCV) na RGB (format MediaPipe)
    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
    
    # Przetw√≥rz klatkƒô przez MediaPipe Face Mesh
    results = face_mesh.process(frame_rgb)

    # Sprawd≈∫ czy wykryto jakƒÖkolwiek twarz
    if results.multi_face_landmarks:
        # Iteruj przez wszystkie wykryte twarze (zazwyczaj jedna)
        for face_landmarks in results.multi_face_landmarks:
            # ANALIZA KIERUNKU SPOJRZENIA
            # ----------------------------
            # Pobierz wsp√≥≈Çrzƒôdne lewego oka (punkt 33 z 468 punkt√≥w siatki)
            left_eye = face_landmarks.landmark[33]
            
            # Pobierz wsp√≥≈Çrzƒôdne prawego oka (punkt 263)
            right_eye = face_landmarks.landmark[263]

            # Okre≈õl kierunek spojrzenia na podstawie pozycji oczu
            if left_eye.x < 0.4:
                # Lewe oko jest bardzo na lewo -> osoba patrzy w lewo
                st.session_state.eye_direction_count["left"] += 1
            elif right_eye.x > 0.6:
                # Prawe oko jest bardzo na prawo -> osoba patrzy w prawo
                st.session_state.eye_direction_count["right"] += 1
            else:
                # Oczy sƒÖ mniej wiƒôcej centralnie -> osoba patrzy prosto
                st.session_state.eye_direction_count["center"] += 1

            # ANALIZA RUCH√ìW G≈ÅOWY
            # ---------------------
            # Pobierz wsp√≥≈Çrzƒôdne czubka nosa (punkt 4) - ≈õrodkowy punkt twarzy
            nose_tip = face_landmarks.landmark[4]
            
            # Okre≈õl pozycjƒô g≈Çowy na podstawie wsp√≥≈Çrzƒôdnej y nosa
            if nose_tip.y < 0.4:
                # Nos jest wysoko -> g≈Çowa podniesiona w g√≥rƒô
                st.session_state.head_movement_count["up"] += 1
            elif nose_tip.y > 0.6:
                # Nos jest nisko -> g≈Çowa opuszczona w d√≥≈Ç
                st.session_state.head_movement_count["down"] += 1
            else:
                # Nos jest mniej wiƒôcej centralnie -> g≈Çowa w pozycji neutralnej
                st.session_state.head_movement_count["still"] += 1

            # Narysuj siatkƒô twarzy na obrazie
            # FACEMESH_CONTOURS to zestaw linii tworzƒÖcych kontur twarzy
            mp_drawing.draw_landmarks(frame, face_landmarks, mp_face_mesh.FACEMESH_CONTOURS)


# FUNKCJA 5: Rozpoczƒôcie analizy wideo (g≈Ç√≥wna pƒôtla programu)
# ==================================================================================
def start_analysis(mode, input_source):
    """
    Uruchamia g≈Ç√≥wnƒÖ pƒôtlƒô analizy wideo z kamery lub pliku.
    
    Parametry:
    ----------
    mode : str
        Tryb analizy: "Detective", "Student Behavior" lub "Interview"
    input_source : str
        ≈πr√≥d≈Ço wideo: "camera" (kamera) lub "video" (plik)
        
    Dzia≈Çanie:
    ----------
    1. Otwiera ≈∫r√≥d≈Ço wideo (kamera lub plik)
    2. Inicjalizuje narzƒôdzia MediaPipe do analizy d≈Çoni i twarzy
    3. Wchodzi w pƒôtlƒô przetwarzania klatek:
       - Odczytuje kolejnƒÖ klatkƒô
       - Analizuje emocje (DeepFace)
       - Analizuje gesty d≈Çoni (MediaPipe)
       - Analizuje kierunek oczu i ruchy g≈Çowy (MediaPipe)
       - Wy≈õwietla wyniki na obrazie
       - Loguje dane do raportu
    4. Po zako≈Ñczeniu generuje raport
    
    Uwaga:
    ------
    Pƒôtla dzia≈Ça dop√≥ki st.session_state.camera_running == True
    U≈ºytkownik zatrzymuje analizƒô przyciskiem "Stop Analysis"
    """
    # KROK 1: Otw√≥rz ≈∫r√≥d≈Ço wideo
    # ----------------------------
    temp_file_path = None  # Zmienna do przechowania ≈õcie≈ºki tymczasowego pliku
    
    if input_source == "camera":
        # Otw√≥rz domy≈õlnƒÖ kamerƒô (0 = pierwsza kamera w systemie)
        cap = cv2.VideoCapture(0)
        
        # Sprawd≈∫ czy kamera zosta≈Ça poprawnie otwarta
        if not cap.isOpened():
            st.error("‚ùå Nie mo≈ºna otworzyƒá kamery. Sprawd≈∫ czy:")
            st.info("‚Ä¢ Kamera jest pod≈ÇƒÖczona\n‚Ä¢ ≈ªadna inna aplikacja nie u≈ºywa kamery\n‚Ä¢ Masz uprawnienia do dostƒôpu do kamery")
            return
    else:
        # Pozw√≥l u≈ºytkownikowi przes≈Çaƒá plik wideo
        file_path = st.file_uploader("Prze≈õlij plik wideo", type=["mp4", "avi", "mov"])
        
        # Sprawd≈∫ czy plik zosta≈Ç przes≈Çany
        if file_path is None:
            st.warning("Proszƒô przes≈Çaƒá plik wideo.")
            return
        
        # Walidacja rozmiaru pliku (max 200 MB)
        max_size_mb = 200
        file_size_mb = file_path.size / (1024 * 1024)  # Konwersja na MB
        
        if file_size_mb > max_size_mb:
            st.error(f"‚ùå Plik jest za du≈ºy ({file_size_mb:.1f} MB). Maksymalny rozmiar: {max_size_mb} MB.")
            st.info("üí° Tip: Skompresuj wideo lub u≈ºyj kr√≥tszego fragmentu.")
            return
        
        # Wy≈õwietl informacjƒô o rozmiarze pliku
        st.info(f"üìÅ Przetwarzanie pliku: {file_path.name} ({file_size_mb:.1f} MB)")
        
        # POPRAWKA: Zapisz przes≈Çany plik tymczasowo
        # Streamlit file_uploader zwraca obiekt UploadedFile, nie ≈õcie≈ºkƒô
        # Musimy zapisaƒá plik tymczasowo, aby OpenCV m√≥g≈Ç go odczytaƒá
        
        try:
            # Utw√≥rz tymczasowy plik z odpowiednim rozszerzeniem
            tfile = tempfile.NamedTemporaryFile(delete=False, suffix=os.path.splitext(file_path.name)[1])
            tfile.write(file_path.read())
            tfile.close()
            temp_file_path = tfile.name  # Zapisz ≈õcie≈ºkƒô do p√≥≈∫niejszego usuniƒôcia
            
            # Otw√≥rz tymczasowy plik
            cap = cv2.VideoCapture(temp_file_path)
            
            # Sprawd≈∫ czy plik zosta≈Ç poprawnie otwarty
            if not cap.isOpened():
                st.error("‚ùå Nie mo≈ºna otworzyƒá pliku wideo. Sprawd≈∫ czy plik jest prawid≈Çowy.")
                if temp_file_path and os.path.exists(temp_file_path):
                    os.unlink(temp_file_path)
                return
        except Exception as e:
            st.error(f"‚ùå B≈ÇƒÖd podczas przetwarzania pliku: {e}")
            if temp_file_path and os.path.exists(temp_file_path):
                os.unlink(temp_file_path)
            return

    # Utw√≥rz pusty kontener Streamlit do wy≈õwietlania wideo
    stframe = st.empty()

    # KROK 2: Zainicjalizuj narzƒôdzia MediaPipe
    # ------------------------------------------
    # U≈ºywamy kontekstu "with" aby automatycznie zwolniƒá zasoby po zako≈Ñczeniu
    with mp_hands.Hands(min_detection_confidence=0.7, min_tracking_confidence=0.7) as hands, \
            mp_face_mesh.FaceMesh(min_detection_confidence=0.7, min_tracking_confidence=0.7) as face_mesh:
        
        # min_detection_confidence=0.7 oznacza, ≈ºe wykrywanie musi mieƒá
        # pewno≈õƒá co najmniej 70%, aby uznaƒá co≈õ za d≈Ço≈Ñ/twarz
        
        # min_tracking_confidence=0.7 oznacza, ≈ºe ≈õledzenie musi mieƒá
        # pewno≈õƒá co najmniej 70%, aby kontynuowaƒá ≈õledzenie tego samego obiektu

        # KROK 3: G≈Ç√≥wna pƒôtla przetwarzania klatek
        # ------------------------------------------
        while st.session_state.camera_running and cap.isOpened():
            # Odczytaj kolejnƒÖ klatkƒô z wideo
            # ret = True je≈õli klatka zosta≈Ça odczytana poprawnie
            # frame = tablica NumPy zawierajƒÖca obraz (klatkƒô)
            ret, frame = cap.read()
            
            # Je≈õli nie uda≈Ço siƒô odczytaƒá klatki (koniec wideo), przerwij pƒôtlƒô
            if not ret:
                break

            # ANALIZA EMOCJI
            # --------------
            # Analizuj emocje widoczne na twarzy w bie≈ºƒÖcej klatce
            emotion_scores = analyze_emotion(frame)

            # Dodaj wyniki emocji do sum ca≈Çkowitych (do obliczenia ≈õredniej p√≥≈∫niej)
            for emotion, score in emotion_scores.items():
                st.session_state.emotion_totals[emotion] += score
            
            # Zwiƒôksz licznik przeanalizowanych klatek
            st.session_state.frame_count += 1

            # ANALIZA GEST√ìW D≈ÅONI
            # ---------------------
            analyze_hands(frame, hands)

            # ANALIZA TWARZY (oczy i g≈Çowa)
            # ------------------------------
            analyze_face(frame, face_mesh)

            # OKRE≈öLENIE DOMINUJƒÑCEJ EMOCJI
            # ------------------------------
            # Znajd≈∫ emocjƒô z najwy≈ºszym wynikiem w bie≈ºƒÖcej klatce
            dominant_emotion = max(emotion_scores, key=emotion_scores.get)
            dominant_emotion_score = emotion_scores[dominant_emotion]

            # WIZUALIZACJA WYNIK√ìW NA OBRAZIE
            # --------------------------------
            # Wy≈õwietl dominujƒÖcƒÖ emocjƒô na g√≥rze obrazu
            emotion_text = f"{dominant_emotion}: {dominant_emotion_score:.2f}%"
            frame = cv2.putText(frame, emotion_text, (50, 50), 
                              cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)
            
            # cv2.putText() parametry:
            # - frame: obraz na kt√≥rym rysujemy
            # - emotion_text: tekst do wy≈õwietlenia
            # - (50, 50): pozycja x, y tekstu
            # - cv2.FONT_HERSHEY_SIMPLEX: typ czcionki
            # - 1: rozmiar czcionki
            # - (0, 255, 0): kolor BGR (zielony)
            # - 2: grubo≈õƒá linii

            # Wy≈õwietl wszystkie emocje z ich procentami
            y_offset = 100  # PoczƒÖtkowa pozycja y dla pierwszej emocji
            for emotion, score in emotion_scores.items():
                frame = cv2.putText(frame, f"{emotion}: {score:.2f}%", 
                                  (50, y_offset), cv2.FONT_HERSHEY_SIMPLEX, 
                                  0.7, (0, 255, 0), 2)
                y_offset += 30  # Przesu≈Ñ pozycjƒô y o 30 pikseli dla kolejnej emocji

            # LOGOWANIE DO RAPORTU
            # --------------------
            # Zapisz wynik analizy do raportu (w zale≈ºno≈õci od trybu)
            if mode == "Detective":
                log_to_report(mode, f"Wykrywanie: {dominant_emotion} ({dominant_emotion_score:.2f}%)")
            elif mode == "Student Behavior":
                log_to_report(mode, f"≈öledzenie: {dominant_emotion} ({dominant_emotion_score:.2f}%)")
            elif mode == "Interview":
                log_to_report(mode, f"Analiza: {dominant_emotion} ({dominant_emotion_score:.2f}%)")

            # WY≈öWIETLENIE KLATKI W APLIKACJI STREAMLIT
            # ------------------------------------------
            # Wy≈õwietl przetworzonƒÖ klatkƒô z na≈Ço≈ºonymi adnotacjami
            stframe.image(frame, channels="BGR", use_container_width=True)
            
            # channels="BGR" - OpenCV u≈ºywa BGR zamiast RGB
            # use_container_width=True - dostosuj szeroko≈õƒá do kontenera

            # Sprawd≈∫ czy naci≈õniƒôto klawisz 'q' (opcjonalne zatrzymanie)
            if cv2.waitKey(10) & 0xFF == ord('q'):
                break

    # KROK 4: Zwolnij zasoby
    # -----------------------
    cap.release()  # Zamknij strumie≈Ñ wideo
    cv2.destroyAllWindows()  # Zamknij wszystkie okna OpenCV
    
    # Usu≈Ñ tymczasowy plik wideo je≈õli zosta≈Ç utworzony
    if temp_file_path is not None:
        try:
            os.remove(temp_file_path)  # Usu≈Ñ plik tymczasowy z dysku
        except Exception as e:
            # Je≈õli nie uda≈Ço siƒô usunƒÖƒá pliku, wy≈õwietl ostrze≈ºenie
            st.warning(f"Nie mo≈ºna usunƒÖƒá pliku tymczasowego: {e}")

    # KROK 5: Wygeneruj raport ko≈Ñcowy
    # ---------------------------------
    generate_report(mode)


# FUNKCJA 6: Generowanie i wy≈õwietlanie raportu ko≈Ñcowego
# ==================================================================================
def generate_report(mode):
    """
    Generuje szczeg√≥≈Çowy raport z analizy emocji i zachowania.
    
    Parametry:
    ----------
    mode : str
        Tryb analizy, kt√≥ry zosta≈Ç u≈ºyty
        
    Dzia≈Çanie:
    ----------
    1. Oblicza ≈õrednie warto≈õci emocji ze wszystkich klatek
    2. Wy≈õwietla statystyki emocji
    3. Wy≈õwietla statystyki gest√≥w, kierunku oczu i ruch√≥w g≈Çowy
    4. Wysy≈Ça dane do Google Gemini AI w celu uzyskania analizy behawioralnej
    5. Wy≈õwietla sugestie AI dotyczƒÖce zachowania
    
    Uwaga:
    ------
    Ta funkcja u≈ºywa API Google Gemini, kt√≥re wymaga klucza API.
    Klucz jest zakodowany na sta≈Çe w kodzie (nie zalecane w produkcji!)
    """
    # KROK 1: Zabezpieczenie przed dzieleniem przez zero
    # ---------------------------------------------------
    if st.session_state.frame_count == 0:
        st.session_state.frame_count = 1

    # KROK 2: Oblicz ≈õrednie warto≈õci emocji
    # ---------------------------------------
    # Dla ka≈ºdej emocji, podziel sumƒô przez liczbƒô klatek
    # Otrzymujemy ≈õredniƒÖ warto≈õƒá procentowƒÖ dla ka≈ºdej emocji w ca≈Çym wideo
    average_emotion_scores = {
        emotion: score / st.session_state.frame_count 
        for emotion, score in st.session_state.emotion_totals.items()
    }

    # KROK 3: Wy≈õwietl nag≈Ç√≥wek raportu
    # ----------------------------------
    st.subheader("Raport Analizy Emocji i Zachowania")
    st.write("=" * 40)

    # KROK 4: Wy≈õwietl wyniki analizy emocji
    # ---------------------------------------
    st.write(f"Tryb analizy: {mode}\n")
    st.write("≈öREDNIE WARTO≈öCI EMOCJI:")
    for emotion, score in average_emotion_scores.items():
        # Wy≈õwietl nazwƒô emocji z wielkiej litery i jej ≈õredni procent
        st.write(f"{emotion.capitalize()}: {score:.2f}%")

    # KROK 5: Wy≈õwietl analizƒô mowy cia≈Ça
    # ------------------------------------
    st.write("\nAnaliza Mowy Cia≈Ça:")
    st.write(f"Napiƒôte Gesty D≈Çoni: {st.session_state.hand_gesture_count['tense']}")
    st.write(f"Rozlu≈∫nione Gesty D≈Çoni: {st.session_state.hand_gesture_count['relaxed']}")
    st.write(f"Kierunek Oczu (Lewo): {st.session_state.eye_direction_count['left']}")
    st.write(f"Kierunek Oczu (Prawo): {st.session_state.eye_direction_count['right']}")
    st.write(f"Kierunek Oczu (Centrum): {st.session_state.eye_direction_count['center']}")
    st.write(f"Ruchy G≈Çowy (G√≥ra): {st.session_state.head_movement_count['up']}")
    st.write(f"Ruchy G≈Çowy (D√≥≈Ç): {st.session_state.head_movement_count['down']}")
    st.write(f"Ruchy G≈Çowy (Nieruchomo): {st.session_state.head_movement_count['still']}")

    # KROK 6: Przygotuj dane do analizy AI
    # -------------------------------------
    # Utw√≥rz tekstowy opis wszystkich zebranych danych
    analysis = ""
    for emotion, score in average_emotion_scores.items():
        analysis += f"{emotion.capitalize()}: {score:.2f}%\n"

    analysis += f"Napiƒôte Gesty D≈Çoni: {st.session_state.hand_gesture_count['tense']}\n"
    analysis += f"Rozlu≈∫nione Gesty D≈Çoni: {st.session_state.hand_gesture_count['relaxed']}\n"
    analysis += f"Kierunek Oczu (Lewo): {st.session_state.eye_direction_count['left']}\n"
    analysis += f"Kierunek Oczu (Prawo): {st.session_state.eye_direction_count['right']}\n"
    analysis += f"Kierunek Oczu (Centrum): {st.session_state.eye_direction_count['center']}\n"
    analysis += f"Ruchy G≈Çowy (G√≥ra): {st.session_state.head_movement_count['up']}\n"
    analysis += f"Ruchy G≈Çowy (D√≥≈Ç): {st.session_state.head_movement_count['down']}\n"
    analysis += f"Ruchy G≈Çowy (Nieruchomo): {st.session_state.head_movement_count['still']}\n"

    # KROK 7: Wywo≈Çaj Google Gemini AI do analizy
    # --------------------------------------------
    # ‚úÖ BEZPIECZNE ROZWIƒÑZANIE - U≈ºycie zmiennych ≈õrodowiskowych lub Streamlit secrets
    # Pr√≥buje za≈Çadowaƒá klucz API w kolejno≈õci:
    # 1. Ze zmiennych ≈õrodowiskowych (GEMINI_API_KEY)
    # 2. Z Streamlit secrets (dla aplikacji wdro≈ºonych na Streamlit Cloud)
    # 3. W przypadku braku klucza - wy≈õwietla ostrze≈ºenie
    
    api_key = None
    
    # Pr√≥ba 1: Zmienne ≈õrodowiskowe
    api_key = os.getenv('GEMINI_API_KEY')
    
    # Pr√≥ba 2: Streamlit secrets (je≈õli aplikacja jest wdro≈ºona)
    if not api_key:
        try:
            api_key = st.secrets.get("gemini_api_key")
        except:
            pass
    
    # Sprawd≈∫ czy klucz API zosta≈Ç znaleziony
    if not api_key:
        st.error("‚ùå Brak klucza API Google Gemini! Ustaw zmiennƒÖ ≈õrodowiskowƒÖ GEMINI_API_KEY lub dodaj klucz do Streamlit secrets.")
        st.info("‚ÑπÔ∏è Jak uzyskaƒá klucz API: https://makersuite.google.com/app/apikey")
        return "Brak konfiguracji API - nie mo≈ºna wygenerowaƒá analizy AI."
    
    # Inicjalizuj klienta z bezpiecznie pobranym kluczem
    try:
        client = genai.Client(api_key=api_key)
    except Exception as e:
        st.error(f"‚ùå B≈ÇƒÖd inicjalizacji klienta Google Gemini: {e}")
        return "B≈ÇƒÖd konfiguracji API - nie mo≈ºna wygenerowaƒá analizy AI."
    
    # Wy≈õlij prompt do modelu Gemini
    response = client.models.generate_content(
        model="gemini-2.0-flash",  # Model AI do u≈ºycia
        contents=f"""Na podstawie tej Analizy Emocji i Zachowania dla trybu {mode}: {analysis}, 
    u≈ºyj formatu JSON do ustrukturyzowania odpowiedzi:

    {{
        "behavior": "opisz og√≥lne zachowanie na podstawie analizy",
        "action": "zasugeruj odpowiednie dzia≈Çanie"
    }}
    """
    )

    # KROK 8: Przetw√≥rz odpowied≈∫ AI
    # -------------------------------
    # Odpowied≈∫ AI jest w formacie JSON, ale czƒôsto opakowana w znaczniki markdown
    # Bezpiecznie wyodrƒôbnij JSON z odpowiedzi
    ans = response.text
    
    # Usu≈Ñ znaczniki markdown je≈õli sƒÖ obecne (np. ```json ... ```)
    # Sprawd≈∫ czy odpowied≈∫ zawiera znaczniki kodu
    if "```" in ans:
        # Znajd≈∫ poczƒÖtek JSON (pierwszy nawias klamrowy)
        start = ans.find("{")
        # Znajd≈∫ koniec JSON (ostatni nawias klamrowy)
        end = ans.rfind("}") + 1
        
        # Sprawd≈∫ czy znaleziono prawid≈Çowe granice JSON
        if start != -1 and end > start:
            ans = ans[start:end]
        else:
            # Je≈õli nie znaleziono prawid≈Çowych granic, spr√≥buj usunƒÖƒá tylko znaczniki ```
            ans = ans.replace("```json", "").replace("```", "").strip()
    
    # Spr√≥buj sparsowaƒá JSON
    try:
        ans = json.loads(ans)  # Parsuj JSON na s≈Çownik Python
    except json.JSONDecodeError as e:
        # Je≈õli parsowanie nie powiedzie siƒô, wy≈õwietl b≈ÇƒÖd i u≈ºyj domy≈õlnych warto≈õci
        st.error(f"Nie uda≈Ço siƒô sparsowaƒá odpowiedzi AI: {e}")
        st.info(f"Otrzymana odpowied≈∫: {response.text[:200]}...")  # Poka≈º fragment odpowiedzi
        ans = {
            "behavior": "Nie mo≈ºna by≈Ço przeanalizowaƒá zachowania z powodu b≈Çƒôdu parsowania JSON",
            "action": "Spr√≥buj ponownie przeprowadziƒá analizƒô"
        }
    
    # KROK 9: Wy≈õwietl analizƒô behawioralnƒÖ AI
    # -----------------------------------------
    st.write("\nAnaliza Behawioralna (wygenerowana przez AI):")
    st.write(f"Zachowanie: {ans['behavior']}")
    st.write(f"Sugerowane dzia≈Çanie: {ans['action']}")

    st.write("\nAnaliza Zako≈Ñczona.")
    st.write("=" * 40)


# ==================================================================================
# SEKCJA 5: INTERFEJS U≈ªYTKOWNIKA STREAMLIT
# ==================================================================================
# Poni≈ºszy kod tworzy interfejs webowy aplikacji przy u≈ºyciu Streamlit.
# Streamlit automatycznie generuje elementy UI na podstawie poni≈ºszych komend.

# NAG≈Å√ìWEK G≈Å√ìWNY APLIKACJI
# --------------------------
st.title("Analizator Emocji i Zachowania")
# Wy≈õwietla du≈ºy, wyr√≥≈ºniony tytu≈Ç na g√≥rze strony

# PANEL BOCZNY (SIDEBAR) - USTAWIENIA
# ------------------------------------
st.sidebar.header("Ustawienia")
# Tworzy nag≈Ç√≥wek w panelu bocznym po lewej stronie

# Element 1: Wyb√≥r trybu analizy
# -------------------------------
mode = st.sidebar.selectbox(
    "Wybierz Tryb Analizy", 
    ["Detective", "Student Behavior", "Interview"]
)
# selectbox tworzy listƒô rozwijanƒÖ (dropdown) z opcjami do wyboru
# Warto≈õƒá wybranej opcji jest zapisywana w zmiennej "mode"
# 
# Wyja≈õnienie tryb√≥w:
# - Detective: Tryb og√≥lny do wykrywania emocji
# - Student Behavior: Tryb do monitorowania uwagi studenta
# - Interview: Tryb do analizy podczas rozmowy kwalifikacyjnej

# Element 2: Wyb√≥r ≈∫r√≥d≈Ça wideo
# ------------------------------
input_source = st.sidebar.radio(
    "Wybierz ≈πr√≥d≈Ço Wideo", 
    ["camera", "video"]
)
# radio tworzy przyciski opcji (radio buttons)
# U≈ºytkownik mo≈ºe wybraƒá tylko jednƒÖ opcjƒô
# 
# Opcje:
# - camera: U≈ºyj kamery internetowej w czasie rzeczywistym
# - video: Prze≈õlij plik wideo z dysku

# Element 3: Przycisk rozpoczƒôcia analizy
# ----------------------------------------
if st.sidebar.button("Rozpocznij Analizƒô"):
    # button tworzy przycisk klikalny
    # Kod wewnƒÖtrz if wykona siƒô tylko gdy u≈ºytkownik kliknie przycisk
    
    # Ustaw flagƒô na True - rozpocznij analizƒô
    st.session_state.camera_running = True
    
    # Wywo≈Çaj funkcjƒô g≈Ç√≥wnƒÖ rozpoczynajƒÖcƒÖ przetwarzanie wideo
    start_analysis(mode, input_source)

# Element 4: Przycisk zatrzymania analizy
# ----------------------------------------
if st.sidebar.button("Zatrzymaj Analizƒô"):
    # Ten przycisk zatrzymuje przetwarzanie wideo
    
    # Ustaw flagƒô na False - zatrzymaj analizƒô
    st.session_state.camera_running = False
    
    # Wy≈õwietl komunikat sukcesu dla u≈ºytkownika
    st.success("Analiza zatrzymana. Generowanie raportu...")
    
    # Wygeneruj i wy≈õwietl raport ko≈Ñcowy
    generate_report(mode)

# ==================================================================================
# KONIEC PROGRAMU
# ==================================================================================
# 
# JAK DZIA≈ÅA STREAMLIT?
# ----------------------
# 1. Streamlit wykonuje ca≈Çy skrypt od g√≥ry do do≈Çu
# 2. Gdy u≈ºytkownik kliknie przycisk/zmieni warto≈õƒá, Streamlit wykonuje skrypt ponownie
# 3. session_state zachowuje dane miƒôdzy wykonaniami (nie tracisz zebranych danych)
# 4. Ka≈ºde polecenie st.* dodaje element do interfejsu w kolejno≈õci wykonania
#
# PRZEP≈ÅYW DZIA≈ÅANIA APLIKACJI:
# ------------------------------
# 1. U≈ºytkownik uruchamia: streamlit run emo.py
# 2. Streamlit otwiera aplikacjƒô w przeglƒÖdarce
# 3. U≈ºytkownik wybiera tryb i ≈∫r√≥d≈Ço wideo
# 4. U≈ºytkownik klika "Rozpocznij Analizƒô"
# 5. Funkcja start_analysis() rozpoczyna przetwarzanie wideo:
#    - Otwiera kamerƒô/plik
#    - W pƒôtli przetwarza kolejne klatki
#    - Analizuje emocje i gesty
#    - Wy≈õwietla wyniki na ≈ºywo
# 6. U≈ºytkownik klika "Zatrzymaj Analizƒô"
# 7. Funkcja generate_report() tworzy raport:
#    - Oblicza ≈õrednie warto≈õci
#    - Wysy≈Ça dane do AI
#    - Wy≈õwietla wyniki i sugestie
#
# WSKAZ√ìWKI DLA STUDENT√ìW:
# -------------------------
# - Spr√≥buj zmieniƒá progi w funkcjach analyze_hands() i analyze_face()
# - Dodaj w≈Çasne emocje lub gesty do wykrywania
# - Zmie≈Ñ kolory tekstu na obrazie (warto≈õci BGR)
# - Dodaj nowe tryby analizy
# - Zapisz raport do pliku zamiast tylko wy≈õwietlaƒá
# - Dodaj wykresy do wizualizacji wynik√≥w (u≈ºyj matplotlib)
#
# ==================================================================================
